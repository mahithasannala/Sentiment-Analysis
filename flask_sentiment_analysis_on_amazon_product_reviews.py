# -*- coding: utf-8 -*-
"""Flask - Sentiment Analysis on Amazon Product Reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OZNVymfgfKaVfBGuZE6lbqH9TO18Z0Y8
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount = True)

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Kaggle

# Getting the used mobile phones dataset from Kaggle
!kaggle datasets download -d PromptCloudHQ/amazon-reviews-unlocked-mobile-phones

# Unzipping the zip files and deleting the zip files
!unzip \*.zip  && rm *.zip

!ls

"""# Parsing Data"""

import csv

filename = open('Amazon_Unlocked_Mobile.csv', 'r')
file = csv.DictReader(filename)

reviews = []
ratings = []

for row in file:
    # filtering out neutral (3 star) reviews
    if (int(row['Rating']) != 3):
      reviews.append(row['Reviews'])
      ratings.append(1 if (int(row['Rating']) > 3) else 0)
    if (len(reviews) == 30000):
      break

# Not using all the reviews to save time while training the model (just to get something up and running), and to leave some for testing purposes

print('Number of reviews: ', len(reviews), '\n')

pos = 0
neg = 0

for sentiment in ratings:
  if (sentiment == 1):
    pos += 1
  else:
    neg += 1

print('Positive reviews: ', pos)
print('Negative reviews: ', neg, '\n')

print('First review: ', reviews[0])
print('Sentiment of the first review (1 if positive, 0 if negative): ', ratings[0])

!pip install transformers

# Importing the necessary libraries

import tensorflow as tf
from tensorflow.keras import activations, optimizers, losses

from tensorflow.keras.layers import *

from transformers import DistilBertTokenizerFast, DistilBertConfig, TFDistilBertModel, TFDistilBertForSequenceClassification
from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig

import numpy as np
import random

"""# Fine-tuning of a pretrained distilBERT model on our dataset"""

# Reference: https://huggingface.co/transformers/custom_datasets.html

# Creating an 80 : 20 training-test split

train_reviews, train_ratings = reviews[:16000], ratings[:16000]
test_reviews, test_ratings = reviews[16000:20000], ratings[16000:20000]

from sklearn.model_selection import train_test_split

# Partitioning some of the training set for validation
train_reviews, val_reviews, train_ratings, val_ratings = train_test_split(train_reviews, train_ratings, test_size = .2)

pos = 0
neg = 0

for rating in test_ratings:
  if (rating == 1):
    pos += 1
  else:
    neg += 1

print('Number of positive reviews in the test set: ', pos)
print('Number of negative reviews in the test set: ', neg)

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_reviews, truncation = True, padding = True)
val_encodings = tokenizer(val_reviews, truncation = True, padding = True)
test_encodings = tokenizer(test_reviews, truncation = True, padding = True)

# Generating the TensorFlow dataset objects

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_ratings
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_ratings
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_ratings
))

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5)
model.compile(optimizer = optimizer, loss = model.compute_loss, metrics = ['accuracy'])

# Fine tuning the model by fitting it on our dataset
model.fit(train_dataset.shuffle(12800).batch(16), epochs = 2, batch_size = 16, validation_data = val_dataset.shuffle(3200).batch(16))

# Saving the fine tuned model
model.save_pretrained("./sentiment")

# Loading the saved model
classifier = TFDistilBertForSequenceClassification.from_pretrained("./sentiment")

# Viewing the architecture of our model
classifier.summary()

# Compiling the loaded model
optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5)
classifier.compile(optimizer = optimizer, loss = classifier.compute_loss, metrics = ['accuracy'])

# Evaluating the classifier on the test dataset
classifier.evaluate(test_dataset.batch(16))

"""# Predictions"""

# Function to perform predictions using the classifier model:

def predict(review):
  input = tokenizer.encode(review, truncation = True, padding = True, return_tensors = 'tf')
  output = classifier.predict(input)[0]
  prediction = tf.nn.softmax(output, axis = 1)
  # Returns 1 if the sentiment is positive, else 0
  return tf.argmax(prediction, axis = 1).numpy()[0]

# Super simple examples for a sanity check
print(predict('Price is good. Everything else is horrible. The software is absolutely horrendous. Nothing works.'))
print(predict('Incredible device despite the steep price. Love the user experience!'))

# A bit more rigorous testing (also using error metrics for skewed classes)

examples = list(zip(reviews, ratings))

# Getting examples which we never used in the training, validation or test sets
examples = examples[20000:]

predict_reviews, actual_ratings = [], []

# Performing predictions on a random 1000 of those samples
for review, rating in random.sample(examples, 1000):
  predict_reviews.append(review)
  actual_ratings.append(rating)

true_pos, false_pos, true_neg, false_neg = 0, 0, 0, 0

for i in range(1000):
  prediction = predict(predict_reviews[i])
  if (prediction == 1):
    if (actual_ratings[i] == 1):
      true_pos += 1
    else:
      false_pos += 1
  else:
    if (actual_ratings[i] == 0):
      true_neg += 1
    else:
      false_neg += 1

print('True positives: ', true_pos)
print('False positives: ', false_pos, '\n')
print('True negatives: ', true_neg)
print('False negatives: ', false_neg, '\n')

# Computing precision / recall metrics
precision = true_pos / (true_pos + false_pos)
recall = true_pos / (true_pos + false_neg)

print('Precision: ', precision)
print('Recall: ', recall, '\n')

# Computing the F1 score
f1_score = 2 * precision * recall / (precision + recall)

print('F1 score (the closer to 1, the better!): ', f1_score)

"""# Phase 2: Weakness Extraction"""

# Ideally, input -> a list of reviews, output -> weaknesses (for now)

# Function to separate the list of reviews into positive and negative lists
def classification(review_list):
  positive_reviews, negative_reviews = [], []
  for review in review_list:
    if (predict(review) == 1):
      positive_reviews.append(review)
    else:
      negative_reviews.append(review)
  return positive_reviews, negative_reviews

# Importing the necessary libraries to process the review text

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import string

# Character mapping that removes punctuation
character_map = str.maketrans('', '', string.punctuation)

# Set of stopwords
stop_words = set(stopwords.words('english'))

# Instantiating a lemmatizer
lemmatizer = WordNetLemmatizer()

# Attempt 1: Crude approach (comparing frequencies of nouns in positive and negative reviews)

# Returns a set of relevant words from each review
def get_word_set(review):
  word_set = set()
  # Splitting the review into sentences
  sentences = nltk.sent_tokenize(review)
  for sentence in sentences:
    # Splitting the sentence into words
    words = nltk.word_tokenize(sentence)
    for word in words:
      # Converting the word to lowercase
      word = word.lower()
      # Removing punctuation (using the character mapping defined in the above cell)
      word = word.translate(character_map)
      # Lemmatizing the word
      word = lemmatizer.lemmatize(word)
    # Removing stop words
    words = [word for word in words if word not in stop_words]
    # Removing numbers
    words = [word for word in words if word.isalpha()]
    # Performing POS tagging
    tagged = nltk.pos_tag(words)
    for word, tag in tagged:
      # If the word was tagged as a noun, add it to the set (convert it into lowercase)!
      if (tag == 'NN'):
        word_set.add(word.lower())
  return word_set

get_word_set("Performance is awful. Heating issues.")

# Classifies the reviews, and gets words which are used relatively more frequently in the negatively classified reviews

def identify_weaknesses(reviews):
  # Performing classification on the reviews
  pos_reviews, neg_reviews = classification(reviews)
  pos_len, neg_len = len(pos_reviews), len(neg_reviews)
  # If there are no negatively classified reviews, we can't identify weaknesses
  if (neg_len == 0):
    print('There are no negative reviews.')
  else:
    weaknesses = []
    pos_freq, neg_freq = {}, {}
    for i in range(pos_len):
      cur_word_set = get_word_set(pos_reviews[i])
      for word in cur_word_set:
        if (word in pos_freq):
          pos_freq[word] += 1
        else:
          pos_freq[word] = 1
    for i in range(neg_len):
      cur_word_set = get_word_set(neg_reviews[i])
      for word in cur_word_set:
        if (word in neg_freq):
          neg_freq[word] += 1
        else:
          neg_freq[word] = 1
    for word in neg_freq:
      if (word in pos_freq):
        # If the relative frequency of the word is much higher in the negatively classified reviews, add it to the output list!
        if (1.5 * pos_freq[word] / pos_len < neg_freq[word] / neg_len):
          weaknesses.append([word, neg_freq[word] / neg_len])
    return weaknesses

# Testing this on some reviews for a phone currently sold on Amazon.

filename = open('PhoneReviews.csv', 'r')
file = csv.DictReader(filename)

phone_reviews = []

for row in file:
  phone_reviews.append(row['review text'])

weaknesses = identify_weaknesses(phone_reviews)
print(weaknesses)

# Observations:
# This review set is super small (~30ish?), so maybe there's a limit to how well this will perform, need a better set!
# Words that shouldn't be on here are there for some reason
# Definitely not the most superior approach

"""# Application using Flask"""

!pip install flask-ngrok

!pip install plotly

import plotly
import plotly.graph_objs as go
import pandas as pd
import numpy as np
import json

def create_plot(res, sol):
    colors = ['green', 'blue', 'yellow', 'red', 'violet', 'orange', 'purple', 'pink', 'brown', 'cyan']
    data = [go.Bar(x = res, y = sol, marker_color = colors)]
    graphJSON = json.dumps(data, cls = plotly.utils.PlotlyJSONEncoder)
    return graphJSON

from flask import Flask, render_template, request
from flask_ngrok import run_with_ngrok

app = Flask(__name__, template_folder = '/content/gdrive/My Drive/template')
APP_ROOT ='/content/gdrive/My Drive'

from operator import itemgetter
import csv
import os

run_with_ngrok(app)   
  
@app.route("/")
def home():
  return render_template('UI.html')


@app.route("/", methods=['POST'])
def result():
  text = request.form['text1']
  sentiment = predict(text)
  f, pred = False, ""
  if (sentiment == 1):
    f = True
    pred = "Positive"
  else:
    f = False
    pred = "Negative"
  return render_template('UI.html', final = f, data = text, result = pred)

@app.route("/upload", methods=["POST"])
def upload():
    target = '/content/gdrive/My Drive/Output'
    print(target)
    if not os.path.isdir(target):
        os.mkdir(target)
    else:
        print("Couldn't create upload directory: {}".format(target))
    print(request.files.getlist("file"))
    for upload in request.files.getlist("file"):
        print(upload)
        print("{} is the file name".format(upload.filename))
        filename = upload.filename
        destination = "/".join([target, filename])
        print("Accept incoming file:", filename)
        print("Save it to:", destination)
        upload.save(destination)
        filename = open(destination, 'r')
        file = csv.DictReader(filename)
        phone_reviews = []
        for row in file:
          phone_reviews.append(row['review text'])
        weaknesses = identify_weaknesses(phone_reviews)
        weaknesses = sorted(weaknesses, key = itemgetter(1), reverse = True)
        res = []
        sol = []
        for i in range(10):
          res.append(weaknesses[i][0])
          sol.append(weaknesses[i][1] * 100)
        bar = create_plot(res, sol)
        
    return render_template("weakness.html", plot = bar)

if __name__ == "__main__":
  app.run()